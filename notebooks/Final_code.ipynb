{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e233f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting with full text preservation...\n",
      "‚úÖ Converted 161 entries\n",
      "üìÅ Saved to: output_2.csv\n",
      "\n",
      "üìã Sample output:\n",
      "0.0,4.0,\"Okay, now let's make it more interactive....\"\n",
      "4.0,6.0,\"Let's take input from the user...\"\n",
      "6.0,9.0,\"rather than us deciding what we want...\"\n",
      "\n",
      "‚úÖ Done! No text lost - everything preserved.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def convert_transcript_to_csv_full(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Convert timestamped transcript to CSV - keeps ALL text intact\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Remove video separator lines\n",
    "    content = re.sub(r'<-+Video\\d+-+>', '', content)\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    # Split by timestamp pattern (M:SS) or (MM:SS)\n",
    "    # Keep the timestamp and everything until the next timestamp\n",
    "    segments = re.split(r'(\\(\\d+:\\d+\\))', content)\n",
    "    \n",
    "    current_time = None\n",
    "    current_text = \"\"\n",
    "    \n",
    "    for segment in segments:\n",
    "        # Check if this is a timestamp\n",
    "        timestamp_match = re.match(r'\\((\\d+):(\\d+)\\)', segment)\n",
    "        \n",
    "        if timestamp_match:\n",
    "            # Save previous entry if exists\n",
    "            if current_time is not None and current_text.strip():\n",
    "                all_rows.append({\n",
    "                    'timestamp': current_time,\n",
    "                    'text': current_text.strip()\n",
    "                })\n",
    "            \n",
    "            # Start new entry\n",
    "            minutes = int(timestamp_match.group(1))\n",
    "            seconds = int(timestamp_match.group(2))\n",
    "            current_time = f\"{minutes}:{seconds:02d}\"\n",
    "            current_text = \"\"\n",
    "        else:\n",
    "            # This is text content\n",
    "            current_text += segment\n",
    "    \n",
    "    # Add last entry\n",
    "    if current_time is not None and current_text.strip():\n",
    "        all_rows.append({\n",
    "            'timestamp': current_time,\n",
    "            'text': current_text.strip()\n",
    "        })\n",
    "    \n",
    "    # Now convert to start_time, end_time format\n",
    "    final_rows = []\n",
    "    \n",
    "    for i, row in enumerate(all_rows):\n",
    "        # Parse timestamp\n",
    "        parts = row['timestamp'].split(':')\n",
    "        start_seconds = int(parts[0]) * 60 + int(parts[1])\n",
    "        \n",
    "        # Get end time from next row\n",
    "        if i + 1 < len(all_rows):\n",
    "            next_parts = all_rows[i + 1]['timestamp'].split(':')\n",
    "            end_seconds = int(next_parts[0]) * 60 + int(next_parts[1])\n",
    "        else:\n",
    "            # Last entry - add 10 seconds\n",
    "            end_seconds = start_seconds + 10\n",
    "        \n",
    "        final_rows.append({\n",
    "            'start_time': float(start_seconds),\n",
    "            'end_time': float(end_seconds),\n",
    "            'text': row['text']\n",
    "        })\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['start_time', 'end_time', 'text'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(final_rows)\n",
    "    \n",
    "    print(f\"‚úÖ Converted {len(final_rows)} entries\")\n",
    "    print(f\"üìÅ Saved to: {output_file}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nüìã Sample output:\")\n",
    "    for i in range(min(3, len(final_rows))):\n",
    "        print(f\"{final_rows[i]['start_time']},{final_rows[i]['end_time']},\\\"{final_rows[i]['text'][:60]}...\\\"\")\n",
    "\n",
    "\n",
    "# ============= ALTERNATIVE: Keep exact timestamps from text =============\n",
    "\n",
    "def convert_with_exact_timestamps(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Alternative: Extract exact timestamps if they exist in text\n",
    "    Format: (MM:SS) text until next timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip video separator lines\n",
    "        if '<---' in line or '--->' in line:\n",
    "            continue\n",
    "        \n",
    "        # Look for timestamp at start of line\n",
    "        match = re.match(r'(\\d+):(\\d+)\\s*(.+)', line.strip())\n",
    "        if match:\n",
    "            minutes = int(match.group(1))\n",
    "            seconds = int(match.group(2))\n",
    "            text = match.group(3)\n",
    "            \n",
    "            start_time = minutes * 60 + seconds\n",
    "            \n",
    "            rows.append({\n",
    "                'start_time': start_time,\n",
    "                'text': text\n",
    "            })\n",
    "    \n",
    "    # Add end times\n",
    "    final_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        if i + 1 < len(rows):\n",
    "            end_time = rows[i + 1]['start_time']\n",
    "        else:\n",
    "            end_time = row['start_time'] + 10\n",
    "        \n",
    "        final_rows.append({\n",
    "            'start_time': float(row['start_time']),\n",
    "            'end_time': float(end_time),\n",
    "            'text': row['text']\n",
    "        })\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['start_time', 'end_time', 'text'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(final_rows)\n",
    "    \n",
    "    print(f\"‚úÖ Converted {len(final_rows)} entries\")\n",
    "    print(f\"üìÅ Saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# ============= USAGE =============\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    INPUT_FILE = 'combine_text.csv'\n",
    "    OUTPUT_FILE = 'output_2.csv'\n",
    "    \n",
    "    # Method 1: Full text preservation\n",
    "    print(\"Converting with full text preservation...\")\n",
    "    convert_transcript_to_csv_full(INPUT_FILE, OUTPUT_FILE)\n",
    "    \n",
    "    print(\"\\n‚úÖ Done! No text lost - everything preserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a0ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\OneDrive\\Desktop\\RAG2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 399 chunks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv('output_2.csv')\n",
    "\n",
    "# Setup splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Chunk text\n",
    "all_chunks = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row['text'])  # Changed to 'text'\n",
    "    \n",
    "    if text and text != 'nan':\n",
    "        chunks = splitter.split_text(text)\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                'chunk_text': chunk,\n",
    "                'chunk_number': chunk_num,\n",
    "                'source_row': idx,\n",
    "                'start_time': row['start_time'],  # Keep timestamp info\n",
    "                'end_time': row['end_time']\n",
    "            })\n",
    "\n",
    "# Save chunks\n",
    "pd.DataFrame(all_chunks).to_csv('chunks_4.csv', index=False)\n",
    "print(f\"‚úì Created {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980ce8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragha\\AppData\\Local\\Temp\\ipykernel_15376\\1599603396.py:2: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 399 chunks...\n",
      "Generating embeddings using Gemini (with rate limiting)...\n",
      "\n",
      "‚úì Batch 1/80 completed (5 embeddings generated)\n",
      "‚úì Batch 2/80 completed (10 embeddings generated)\n",
      "‚úì Batch 3/80 completed (15 embeddings generated)\n",
      "‚úì Batch 4/80 completed (20 embeddings generated)\n",
      "‚úì Batch 5/80 completed (25 embeddings generated)\n",
      "‚úì Batch 6/80 completed (30 embeddings generated)\n",
      "‚úì Batch 7/80 completed (35 embeddings generated)\n",
      "‚úì Batch 8/80 completed (40 embeddings generated)\n",
      "‚úì Batch 9/80 completed (45 embeddings generated)\n",
      "‚úì Batch 10/80 completed (50 embeddings generated)\n",
      "‚úì Batch 11/80 completed (55 embeddings generated)\n",
      "‚úì Batch 12/80 completed (60 embeddings generated)\n",
      "‚úì Batch 13/80 completed (65 embeddings generated)\n",
      "‚úì Batch 14/80 completed (70 embeddings generated)\n",
      "‚úì Batch 15/80 completed (75 embeddings generated)\n",
      "‚úì Batch 16/80 completed (80 embeddings generated)\n",
      "‚úì Batch 17/80 completed (85 embeddings generated)\n",
      "‚úì Batch 18/80 completed (90 embeddings generated)\n",
      "‚úì Batch 19/80 completed (95 embeddings generated)\n",
      "‚úì Batch 20/80 completed (100 embeddings generated)\n",
      "‚úì Batch 21/80 completed (105 embeddings generated)\n",
      "‚ö†Ô∏è  Rate limit hit. Waiting 10 seconds... (Retry 1/5)\n",
      "‚ö†Ô∏è  Rate limit hit. Waiting 60 seconds... (Retry 2/5)\n",
      "‚úì Batch 22/80 completed (110 embeddings generated)\n",
      "‚úì Batch 23/80 completed (115 embeddings generated)\n",
      "‚úì Batch 24/80 completed (120 embeddings generated)\n",
      "‚úì Batch 25/80 completed (125 embeddings generated)\n",
      "‚úì Batch 26/80 completed (130 embeddings generated)\n",
      "‚úì Batch 27/80 completed (135 embeddings generated)\n",
      "‚úì Batch 28/80 completed (140 embeddings generated)\n",
      "‚úì Batch 29/80 completed (145 embeddings generated)\n",
      "‚úì Batch 30/80 completed (150 embeddings generated)\n",
      "‚úì Batch 31/80 completed (155 embeddings generated)\n",
      "‚úì Batch 32/80 completed (160 embeddings generated)\n",
      "‚úì Batch 33/80 completed (165 embeddings generated)\n",
      "‚úì Batch 34/80 completed (170 embeddings generated)\n",
      "‚úì Batch 35/80 completed (175 embeddings generated)\n",
      "‚úì Batch 36/80 completed (180 embeddings generated)\n",
      "‚úì Batch 37/80 completed (185 embeddings generated)\n",
      "‚úì Batch 38/80 completed (190 embeddings generated)\n",
      "‚úì Batch 39/80 completed (195 embeddings generated)\n",
      "‚úì Batch 40/80 completed (200 embeddings generated)\n",
      "‚úì Batch 41/80 completed (205 embeddings generated)\n",
      "‚úì Batch 42/80 completed (210 embeddings generated)\n",
      "‚úì Batch 43/80 completed (215 embeddings generated)\n",
      "‚úì Batch 44/80 completed (220 embeddings generated)\n",
      "‚úì Batch 45/80 completed (225 embeddings generated)\n",
      "‚úì Batch 46/80 completed (230 embeddings generated)\n",
      "‚úì Batch 47/80 completed (235 embeddings generated)\n",
      "‚úì Batch 48/80 completed (240 embeddings generated)\n",
      "‚úì Batch 49/80 completed (245 embeddings generated)\n",
      "‚úì Batch 50/80 completed (250 embeddings generated)\n",
      "‚úì Batch 51/80 completed (255 embeddings generated)\n",
      "‚úì Batch 52/80 completed (260 embeddings generated)\n",
      "‚ö†Ô∏è  Rate limit hit. Waiting 7 seconds... (Retry 1/5)\n",
      "‚ö†Ô∏è  Rate limit hit. Waiting 60 seconds... (Retry 2/5)\n",
      "‚úì Batch 53/80 completed (265 embeddings generated)\n",
      "‚úì Batch 54/80 completed (270 embeddings generated)\n",
      "‚úì Batch 55/80 completed (275 embeddings generated)\n",
      "‚úì Batch 56/80 completed (280 embeddings generated)\n",
      "‚úì Batch 57/80 completed (285 embeddings generated)\n",
      "‚úì Batch 58/80 completed (290 embeddings generated)\n",
      "‚úì Batch 59/80 completed (295 embeddings generated)\n",
      "‚úì Batch 60/80 completed (300 embeddings generated)\n",
      "‚úì Batch 61/80 completed (305 embeddings generated)\n",
      "‚úì Batch 62/80 completed (310 embeddings generated)\n",
      "‚úì Batch 63/80 completed (315 embeddings generated)\n",
      "‚úì Batch 64/80 completed (320 embeddings generated)\n",
      "‚úì Batch 65/80 completed (325 embeddings generated)\n",
      "‚úì Batch 66/80 completed (330 embeddings generated)\n",
      "‚úì Batch 67/80 completed (335 embeddings generated)\n",
      "‚úì Batch 68/80 completed (340 embeddings generated)\n",
      "‚úì Batch 69/80 completed (345 embeddings generated)\n",
      "‚úì Batch 70/80 completed (350 embeddings generated)\n",
      "‚úì Batch 71/80 completed (355 embeddings generated)\n",
      "‚úì Batch 72/80 completed (360 embeddings generated)\n",
      "‚úì Batch 73/80 completed (365 embeddings generated)\n",
      "‚úì Batch 74/80 completed (370 embeddings generated)\n",
      "‚úì Batch 75/80 completed (375 embeddings generated)\n",
      "‚úì Batch 76/80 completed (380 embeddings generated)\n",
      "‚úì Batch 77/80 completed (385 embeddings generated)\n",
      "‚úì Batch 78/80 completed (390 embeddings generated)\n",
      "‚úì Batch 79/80 completed (395 embeddings generated)\n",
      "‚úì Batch 80/80 completed (399 embeddings generated)\n",
      "\n",
      "‚úì Successfully generated 399 embeddings\n",
      "‚úì Embedding dimension: 3072\n",
      "‚úì Saved to chunks_with_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# ================= CONFIG ================= #\n",
    "genai.configure(api_key=\"AIzaSyBBJ-NRM9xKQB3NclSpMzA_3ZYOY75Fvps\")\n",
    "\n",
    "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
    "\n",
    "# ================= LOAD DATA ================= #\n",
    "df = pd.read_csv(\"chunks_4.csv\")\n",
    "chunks_text = df[\"chunk_text\"].astype(str).tolist()\n",
    "\n",
    "print(f\"Processing {len(chunks_text)} chunks...\")\n",
    "print(\"Generating embeddings using Gemini (with rate limiting)...\\n\")\n",
    "\n",
    "# ================= EMBEDDING FUNCTION WITH RATE LIMITING ================= #\n",
    "def generate_embeddings(texts, batch_size=5):  # Reduced batch size\n",
    "    all_embeddings = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        retry_count = 0\n",
    "        max_retries = 5\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = genai.embed_content(\n",
    "                    model=EMBEDDING_MODEL,\n",
    "                    content=batch,\n",
    "                    task_type=\"retrieval_document\"\n",
    "                )\n",
    "                \n",
    "                all_embeddings.extend(response[\"embedding\"])\n",
    "                print(f\"‚úì Batch {batch_num}/{total_batches} completed ({len(all_embeddings)} embeddings generated)\")\n",
    "                \n",
    "                # Wait longer to avoid rate limits (1 second per batch)\n",
    "                time.sleep(1.0)\n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except exceptions.ResourceExhausted as e:\n",
    "                retry_count += 1\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"\\n‚ùå Max retries reached. Stopping.\")\n",
    "                    raise\n",
    "                \n",
    "                # Extract wait time from error message\n",
    "                wait_time = 60  # Default wait time\n",
    "                if \"retry in\" in str(e):\n",
    "                    try:\n",
    "                        wait_str = str(e).split(\"retry in \")[1].split(\"s\")[0]\n",
    "                        wait_time = float(wait_str) + 2  # Add buffer\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                print(f\"‚ö†Ô∏è  Rate limit hit. Waiting {wait_time:.0f} seconds... (Retry {retry_count}/{max_retries})\")\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error: {e}\")\n",
    "                raise\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# ================= RUN ================= #\n",
    "embeddings = generate_embeddings(chunks_text)\n",
    "\n",
    "print(f\"\\n‚úì Successfully generated {len(embeddings)} embeddings\")\n",
    "print(f\"‚úì Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "# ================= SAVE EMBEDDINGS ================= #\n",
    "df['embedding'] = embeddings\n",
    "df.to_csv(\"chunks_with_embeddings.csv\", index=False)\n",
    "print(\"‚úì Saved to chunks_with_embeddings.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deleted old collection\n",
      "‚úì Stored 399 embeddings in ChromaDB\n",
      "‚úì Embedding dimension: 3072\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# ================= SETUP CHROMADB ================= #\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# ‚úÖ DELETE OLD COLLECTION (if it exists)\n",
    "try:\n",
    "    client.delete_collection(name=\"video_chunks_2\")\n",
    "    print(\"‚úì Deleted old collection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ‚úÖ CREATE NEW COLLECTION (it will auto-detect 3072 dimensions)\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"video_chunks_2\"\n",
    ")\n",
    "\n",
    "# ================= ADD TO DATABASE ================= #\n",
    "collection.add(\n",
    "    documents=chunks_text,\n",
    "    embeddings=embeddings,  # 3072-dimensional embeddings from Gemini\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks_text))],\n",
    "    metadatas=[\n",
    "        {\n",
    "            \"start_time\": row[\"start_time\"],\n",
    "            \"end_time\": row[\"end_time\"]\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Stored {len(embeddings)} embeddings in ChromaDB\")\n",
    "print(f\"‚úì Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b1401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the example of arithmetic operator in Java? and PIR Motion\n",
      "\n",
      "Result 1:\n",
      "<--------------code-box video 24 ------------------------------------------------------------------->\n",
      "PIR motion sensor. \n",
      "00:07\n",
      "The PIR motion sensor uses the RE-200 BP sensing element. \n",
      "00:12\n",
      "It works based on the pyroelectric effect, which means it can detect infrared radiation naturally released.\n",
      "00:20\n",
      "By a human body or an animal, with the help of a Fresnel lens, the sensor can detect motion from a greater distance and over a wider area. \n",
      "00:30\n",
      "When a person or an animal moves within its sensing range, the PIR sensor outputs a high signal. \n",
      "00:37\n",
      "When no motion is present, it outputs a low signal.\n",
      "00:41\n",
      "The programming blocks shown demonstrate how to read the PIR sensor value and use it inside an IF and else structure. \n",
      "00:49\n",
      "The code checks whether motion is detected. \n",
      "00:51\n",
      "If the condition is true, the instructions inside the IF section will run. \n",
      "00:56\n",
      "If the condition is false, the code inside the L section will run.\n",
      "01:02\n",
      "--------------------------------------------------\n",
      "Result 2:\n",
      "02:32\n",
      "The forever loop continuously checks for movement. \n",
      "02:35\n",
      "If motion is detected by the PIR sensor, the system prints the word someone in the serial monitor.\n",
      "02:44\n",
      "If no movement is detected, the system prints the message No one. \n",
      "02:48\n",
      "This completed code allows us to clearly observe how the PIR sensor responds in real time. \n",
      "02:54\n",
      "When we upload the code to Arduino and open the serial monitor, we can see the live output as a person moves in front of the.\n",
      "03:04\n",
      "Sensor the serial monitor prints someone. \n",
      "03:06\n",
      "When no motion is detected, the serial monitor prints no one. \n",
      "03:11\n",
      "This helps verify that the sensor and the code are working correctly.\n",
      "<------------------------------Code-box video 25 ------------------------------------------------------------------->\n",
      "Photo resistor is a photoelectric device that operates according to the principle of semiconductor photoconductivity. \n",
      "00:12\n",
      "--------------------------------------------------\n",
      "Result 3:\n",
      "139.34,154.3,Now by let again no errors let us run it.\n",
      "154.3,156.74,I think the terminal is open.\n",
      "156.74,161.58,So no difference.\n",
      "161.58,166.74,So that would be assignment operators.\n",
      "166.74,173.62,So the other operators would also work in a similar way it will subtract multiply divide\n",
      "173.62,176.3,finding modulus and everything.\n",
      "176.3,180.5,Now next we have increment and decrement.\n",
      "180.5,182.74,Let us have a look at this.\n",
      "182.74,185.9,This is a very interesting operator.\n",
      "185.9,188.1,So let us see.\n",
      "188.1,196.7,Now here what I do is I simply write it is a unary operator means we can\n",
      "196.7,202.38,use it with a single variable we do not need a bc equal to nothing.\n",
      "202.38,210.9,Now a plus plus what this would do is it will increment the value by 1.\n",
      "210.9,216.06,So it will add 1 to it.\n",
      "216.06,220.42,Let us see just yeah.\n",
      "220.42,223.98,Okay let us have a look at it.\n",
      "224.02,231.42,So the original value was 12 it has incremented it to 13.\n",
      "231.42,234.5,But there is a difference.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import chromadb\n",
    "\n",
    "# ================= CONFIG ================= #\n",
    "genai_client = genai.Client(api_key=\"AIzaSyBBJ-NRM9xKQB3NclSpMzA_3ZYOY75Fvps\")\n",
    "EMBED_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./video_db_2\")\n",
    "collection = client.get_collection(\"video_chunks_2\")\n",
    "\n",
    "# ================= QUERY ================= #\n",
    "query = \"What is the example of arithmetic operator in Java? and PIR Motion\"\n",
    "\n",
    "# Generate query embedding (Gemini)\n",
    "response = genai_client.models.embed_content(\n",
    "    model=EMBED_MODEL,\n",
    "    contents=query\n",
    ")\n",
    "\n",
    "query_embedding = response.embeddings[0].values  # list[float]\n",
    "\n",
    "# ================= SEARCH ================= #\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],  # ‚ùå no .tolist()\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# ================= PRINT RESULTS ================= #\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "\n",
    "for i, doc in enumerate(results[\"documents\"][0]):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cd59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 569.05it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:08<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Deleted old collection with wrong dimensions\n",
      "‚úì Database ready with 399 embeddings!\n",
      "‚úì Embedding dimension: 384\n",
      "\n",
      "üîç Searching for: Can you explain the Bluetooth topic mentioned in the video?\n",
      "\n",
      "üìö Retrieved chunks:\n",
      "  1. When we send one from the mobile phone, the Arduino turns the LED on. \n",
      "01:06\n",
      "When we send 0, the Ard...\n",
      "  2. Hello everyone, welcome to Wiz Robo. \n",
      "00:09\n",
      "Today we are going to learn a very exciting project. \n",
      "00...\n",
      "  3. 02:01\n",
      "It helps students understand serial communication, Bluetooth technology, and digital output co...\n",
      "\n",
      "ü§ñ Generating answer...\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "The video discusses Bluetooth technology in the context of controlling devices wirelessly using Arduino and a smartphone. Specifically, it mentions two projects: \n",
      "\n",
      "1. A Bluetooth controlled LED system, where sending \"1\" from a mobile phone turns the LED on and sending \"0\" turns it off. \n",
      "2. A Bluetooth controlled RGB LED system, where an HC05 Bluetooth module is used to send commands from a mobile phone to the Arduino, allowing for wireless control of different colors.\n",
      "\n",
      "The projects demonstrate how Bluetooth technology can be used for wireless communication and control of electronic devices, which has applications in real-life scenarios such as home automation, smart lights, and remote-controlled appliances.\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The video discusses Bluetooth technology in the context of controlling devices wirelessly using Arduino and a smartphone. Specifically, it mentions two projects: \\n\\n1. A Bluetooth controlled LED system, where sending \"1\" from a mobile phone turns the LED on and sending \"0\" turns it off. \\n2. A Bluetooth controlled RGB LED system, where an HC05 Bluetooth module is used to send commands from a mobile phone to the Arduino, allowing for wireless control of different colors.\\n\\nThe projects demonstrate how Bluetooth technology can be used for wireless communication and control of electronic devices, which has applications in real-life scenarios such as home automation, smart lights, and remote-controlled appliances.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from .env file\n",
    "API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# ============= SETUP DATABASE =============\n",
    "\n",
    "df = pd.read_csv('chunks_4.csv')\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "chunks_text = df['chunk_text'].tolist()\n",
    "embeddings = model.encode(chunks_text, show_progress_bar=True)\n",
    "\n",
    "# ‚úÖ DELETE OLD COLLECTION FIRST\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "try:\n",
    "    client.delete_collection(\"video_chunks_2\")\n",
    "    print(\"‚úì Deleted old collection with wrong dimensions\")\n",
    "except:\n",
    "    print(\"‚úì No existing collection to delete\")\n",
    "\n",
    "# ‚úÖ CREATE FRESH COLLECTION\n",
    "collection = client.get_or_create_collection(\"video_chunks_2\")\n",
    "\n",
    "collection.add(\n",
    "    documents=chunks_text,\n",
    "    embeddings=embeddings.tolist(),\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks_text))],\n",
    "    metadatas=[{\"start_time\": row['start_time'], \"end_time\": row['end_time']} \n",
    "               for _, row in df.iterrows()]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Database ready with {len(embeddings)} embeddings!\")\n",
    "print(f\"‚úì Embedding dimension: {embeddings.shape[1]}\")\n",
    "\n",
    "# ============= RAG QUERY FUNCTION =============\n",
    "\n",
    "def rag_query(question, top_k=3):\n",
    "    \n",
    "    print(f\"\\nüîç Searching for: {question}\")\n",
    "    query_embedding = model.encode([question])[0]\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "    \n",
    "    print(\"\\nüìö Retrieved chunks:\")\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"  {i+1}. {doc[:100]}...\")\n",
    "    \n",
    "    print(\"\\nü§ñ Generating answer...\")\n",
    "    \n",
    "    groq_client = Groq(api_key=API_KEY)\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that answers questions based on video transcripts.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Based on the following video transcript excerpts, answer the question.\n",
    "\n",
    "Context from video:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based only on the information provided above.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° ANSWER:\")\n",
    "    print(\"=\"*70)\n",
    "    print(answer)\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# ============= USE IT =============\n",
    "\n",
    "rag_query(\"Can you explain the Bluetooth topic mentioned in the video?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f124a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ragha\\OneDrive\\Desktop\\RAG2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\ragha\\AppData\\Local\\Temp\\ipykernel_50148\\2646940289.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-2.5-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-pro ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-exp-image-generation ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash-lite-001 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.0-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-exp-1206 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-tts ['countTokens', 'generateContent']\n",
      "models/gemini-2.5-pro-preview-tts ['countTokens', 'generateContent', 'batchGenerateContent']\n",
      "models/gemma-3-1b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e4b-it ['generateContent', 'countTokens']\n",
      "models/gemma-3n-e2b-it ['generateContent', 'countTokens']\n",
      "models/gemini-flash-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-flash-lite-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-pro-latest ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-image ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-2.5-flash-lite-preview-09-2025 ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-flash-preview ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "models/gemini-3-pro-image-preview ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/nano-banana-pro-preview ['generateContent', 'countTokens', 'batchGenerateContent']\n",
      "models/gemini-robotics-er-1.5-preview ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-computer-use-preview-10-2025 ['generateContent', 'countTokens']\n",
      "models/deep-research-pro-preview-12-2025 ['generateContent', 'countTokens']\n",
      "models/gemini-embedding-001 ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
      "models/aqa ['generateAnswer']\n",
      "models/imagen-4.0-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-ultra-generate-preview-06-06 ['predict']\n",
      "models/imagen-4.0-generate-001 ['predict']\n",
      "models/imagen-4.0-ultra-generate-001 ['predict']\n",
      "models/imagen-4.0-fast-generate-001 ['predict']\n",
      "models/veo-2.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-generate-001 ['predictLongRunning']\n",
      "models/veo-3.0-fast-generate-001 ['predictLongRunning']\n",
      "models/veo-3.1-generate-preview ['predictLongRunning']\n",
      "models/veo-3.1-fast-generate-preview ['predictLongRunning']\n",
      "models/gemini-2.5-flash-native-audio-latest ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025 ['countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025 ['countTokens', 'bidiGenerateContent']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyB9DbuTKH4HAZXBtNvUtR4VvPxaXt4E5iI\")\n",
    "\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name, model.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c6384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f438f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
